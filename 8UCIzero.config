# Number of worker threads default 2
threadsCPU=3

# weight file default: <autodiscover>
weights=<autodiscover>

# Minibatch size for NN inference defaul: 256, min: 1, max: 1024  512 for 2080ti
minibatch-size=176

# Max prefetch nodes, per NN call defaul: 32, min: 0, max: 1024
max-prefetch=160

# Cpuct MCTS option defaul: 3.4f min: 0.0f, max: 100.0f, 2.5f
# cpuct=3.4f

# fpu-value=1.1f
# cpuct-factor=2.30f
# cpuct-base=10000


# Display verbose move stats default: false
verbose-move-stats=false

multipv=1


# Length of history to include in cache default: 1 max: 7
# cache-history-length=0

# Policy softmax temperature defaul: 2.2f
policy-softmax-temp=2.2f

# Allowed node collisions, per batch  defaul: 1 max: 1024
max-collision-events=917   

# NNCache size defaul: 3200000, max: 999999999
nncache=51200000

# NN backend to use defaul: cudnn  multiplexing  backend=cudnn-fp16 backend=cudnn  cudnn-auto demux
backend=demux

# NN backend parameters default: (backend=cudnn,gpu=0) (backend=cudnn,gpu=0),(backend=cudnn,gpu=1),(backend=cudnn,gpu=2),(backend=cudnn,gpu=3)
# --backend=roundrobin --backend-opts=(backend=cudnn-auto,gpu=0),(backend=cudnn-auto,gpu=1)
# backend-opts=(backend=cudnn-fp16,gpu=0),(backend=cudnn-fp16,gpu=1),(backend=cudnn-fp16,gpu=2),(backend=cudnn-fp16,gpu=3),(backend=cudnn-fp16,gpu=4),(backend=cudnn-fp16,gpu=5),(backend=cudnn-fp16,gpu=6),(backend=cudnn-fp16,gpu=7)
# backend-opts=(backend=cudnn,gpu=0)

# engine can use openbook default: false
usebook=false


out-of-order-eval=true
max-out-of-order-evals-factor=2.4


# Move time overhead in milliseconds defaul: 200
move-overhead=500

# Ponder think defaul: true
#ponder=true

moves-left-max-effect=0.1
moves-left-slope=0.007
moves-left-threshold=0.862976
smart-pruning-minimum-batches=320


